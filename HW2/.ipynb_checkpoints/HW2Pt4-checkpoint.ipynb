{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45a189cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Isaac\\anaconda3\\envs\\CS273\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import os, glob\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from skorch import NeuralNetClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df99faec",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "CNN tuning: Focus on simulation setting 7 to optimize your CNN for improved \n",
    "classification performance on the test set. Generate an independent validation set of 1000 \n",
    "subjects to assist in tuning. Define a search space for the number of convolution layers, \n",
    "the width of the final fully connected layer, and the optimizer's learning rate. Conduct a \n",
    "grid search over the defined search space to identify the best hyperparameter \n",
    "combination. Report the search space, selected hyperparameters, and the classification \n",
    "accuracy for each combination tested.\n",
    "\n",
    "* Use model a base model.\n",
    "    * Remember only using data setting 7. [X]\n",
    "    * have only one train (n_i), one validation (1000), and one test set (1000) [X]\n",
    "    * number of convolution layers: 1, 2, 3, 4, 5\n",
    "    * Readjust model with different final fully connected layers: 8, 16, 32, 64, 128\n",
    "    * lr = 0.001, .01, .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9cbdbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulateData(n, mu_c, mu_n):\n",
    "\n",
    "    y = np.random.choice([0, 1], size = n, p = [0.5, 0.5])\n",
    "    m_i = np.random.poisson(lam = mu_c, size = n) * y + np.random.poisson(lam = mu_n, size = n) * (1 - y)\n",
    "\n",
    "    simulated_data = np.zeros([n, 32, 32])\n",
    "    for i in range(n):\n",
    "        random_indices = np.random.choice(32 * 32, m_i[i], replace = False)\n",
    "        row_indices, col_indices = np.unravel_index(random_indices, (32, 32))\n",
    "        Bi = np.zeros([32, 32])\n",
    "        Bi[row_indices, col_indices] = 1\n",
    "        epsilon_i = np.random.normal(loc = 0, scale = np.sqrt(0.04), size = (32, 32))\n",
    "        simulated_data[i] = Bi + epsilon_i\n",
    "\n",
    "    return y, simulated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0e1b8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = simulateData(n = 200, mu_c = 5, mu_n = 30)\n",
    "y_val, X_val = simulateData(n = 1000, mu_c = 5, mu_n = 30)\n",
    "y_test, X_test = simulateData(n = 1000, mu_c = 5, mu_n = 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "130fc618",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataSetPytorch(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.from_numpy(x.reshape([-1, 1, 32, 32])).float()\n",
    "        self.y = torch.from_numpy(y)\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "def reset_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            init.xavier_uniform_(m.weight)\n",
    "    return\n",
    "\n",
    "def saveModel(model, path):\n",
    "    torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ac6f2fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 32, 32])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exampleImg = X_train[0, :, :].reshape([1, 1, 32, 32])\n",
    "exampleImg = torch.from_numpy(exampleImg).float()\n",
    "exampleImg.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8d031a",
   "metadata": {},
   "source": [
    "#### Model Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e48aabd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.973, dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the base model\n",
    "class BaseCNN(torch.nn.Module):\n",
    "    def __init__(self, num_conv_layers, final_fc_width):\n",
    "        super(BaseCNN, self).__init__()\n",
    "        \n",
    "        self.num_conv_layers = num_conv_layers\n",
    "        \n",
    "        # Define convolutional layers dynamically based on num_conv_layers\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        \n",
    "        in_channels = 1  #  input channels = 1\n",
    "        out_channels = 2  # Initial output channels\n",
    "        for i in range(num_conv_layers):\n",
    "            self.conv_layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "            self.conv_layers.append(nn.ReLU())\n",
    "            self.conv_layers.append(nn.MaxPool2d(kernel_size=2))\n",
    "            # Update in_channels and out_channels for the next layer\n",
    "            in_channels = out_channels\n",
    "            out_channels *= 2  # Double the channels for each subsequent layer\n",
    "        \n",
    "        # Calculate the input size for the fully connected layer\n",
    "        conv_output_size = self._get_conv_output_size()\n",
    "        \n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.fc = nn.Linear(conv_output_size, final_fc_width)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(final_fc_width, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass through convolutional layers\n",
    "        for conv_layer in self.conv_layers:\n",
    "            x = conv_layer(x)\n",
    "        \n",
    "        # Flatten the output from convolutional layers\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        # Forward pass through fully connected layers\n",
    "        x = self.relu(self.fc(x))\n",
    "        x = self.output_layer(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    def _get_conv_output_size(self):\n",
    "        # Method to calculate the output size of the convolutional layers\n",
    "        x = torch.rand(1, 1, 32, 32)  # Assuming input size is 28x28\n",
    "        for conv_layer in self.conv_layers:\n",
    "            x = conv_layer(x)\n",
    "        return x.view(1, -1).size(1)\n",
    "    \n",
    "    def reset_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                init.xavier_uniform_(m.weight)\n",
    "        return\n",
    "    \n",
    "    def fit(self, X_train, X_val, y_train, y_val, learningRate, num_epochs = 200):        \n",
    "\n",
    "        datasetSetting_train = dataSetPytorch(X_train, y_train)\n",
    "        train_dl = DataLoader(datasetSetting_train, batch_size=25, shuffle = True)\n",
    "\n",
    "        datasetSetting_val = dataSetPytorch(X_val, y_val)\n",
    "        valid_dl = DataLoader(datasetSetting_val, batch_size=25, shuffle = True)\n",
    "\n",
    "        # reinitialize weights!\n",
    "        self.reset_weights()\n",
    "\n",
    "        loss_fn = torch.nn.BCELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = learningRate)\n",
    "        loss_hist_train = [0] * num_epochs\n",
    "        accuracy_hist_train = [0] * num_epochs\n",
    "        loss_hist_valid = [0] * num_epochs\n",
    "        accuracy_hist_valid = [0] * num_epochs \n",
    "\n",
    "        best_loss = torch.inf\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "\n",
    "            for x_batch, y_batch in train_dl:\n",
    "                pred = self(x_batch)[:, 0]\n",
    "                loss = loss_fn(pred, y_batch.float())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                loss_hist_train[epoch] += loss.item() * y_batch.size(0)\n",
    "\n",
    "                is_correct = ((pred >= 0.5).float() == y_batch).float()\n",
    "                accuracy_hist_train[epoch] += is_correct.sum()\n",
    "            loss_hist_train[epoch] /= len(train_dl.dataset)\n",
    "            accuracy_hist_train[epoch] /= len(train_dl.dataset)\n",
    "\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                for x_batch, y_batch in valid_dl:\n",
    "                    pred = self(x_batch)[:, 0]\n",
    "                    loss = loss_fn(pred, y_batch.float())\n",
    "                    loss_hist_valid[epoch] += loss.item() * y_batch.size(0)\n",
    "\n",
    "                    is_correct = ((pred >= 0.5).float() == y_batch).float()\n",
    "                    accuracy_hist_valid[epoch] += is_correct.sum()\n",
    "            loss_hist_valid[epoch] /= len(valid_dl.dataset)\n",
    "            accuracy_hist_valid[epoch] /= len(valid_dl.dataset)\n",
    "\n",
    "            if (best_loss > loss_hist_valid[epoch]):\n",
    "                saveModel(model, \"bestModel.pth\")\n",
    "                best_loss = loss_hist_valid[epoch]\n",
    "                \n",
    "        modelWeightParams = torch.load(\"bestModel.pth\")\n",
    "        self.load_state_dict(modelWeightParams)\n",
    "        \n",
    "    def predict_score(self, X_test, y_test):\n",
    "        datasetSetting_test = dataSetPytorch(X_test, y_test)\n",
    "        test_dl = DataLoader(datasetSetting_test, batch_size=25, shuffle = True)\n",
    "\n",
    "        # model is done training, now evaluate the test accuracy score here.\n",
    "        accuracy_test = 0\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in test_dl:\n",
    "                pred = self(x_batch)[:, 0]\n",
    "                is_correct = ((pred >= 0.5).float() == y_batch).float()\n",
    "                accuracy_test += is_correct.sum()   \n",
    "        accuracy_test /= len(test_dl.dataset)\n",
    "\n",
    "        return accuracy_test.numpy()       \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2402876",
   "metadata": {},
   "source": [
    "### Grid Search.\n",
    "Im making my own custom one, oculdnt figure it out with sklearn. Im going to continue what i did and select best model based on best validation performance, since i care most about accuracy. Then im going to run the model on the test data set. Im going to attempt all possible combinations here, and I will select the best model with whatever parameters performed best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f478263",
   "metadata": {},
   "source": [
    "Trial run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b49ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "param_dict = {\n",
    "    'final_fc_width': 8,\n",
    "    'num_conv_layers': 1,\n",
    "    'learningRate': 0.001,\n",
    "}\n",
    "\n",
    "# Create an instance of MyModel\n",
    "model = BaseCNN(num_conv_layers = param_dict[\"num_conv_layers\"], final_fc_width = param_dict[\"final_fc_width\"])\n",
    "model.fit(X_train, X_val, y_train, y_val, \n",
    "          param_dict['learningRate'], num_epochs = 200)\n",
    "model.predict_score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56cbafe",
   "metadata": {},
   "source": [
    "Full send it ma boi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cfa79c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running final_fc_width 8 num_conv_layers 1 learningRate 0.001\n",
      "Running final_fc_width 8 num_conv_layers 1 learningRate 0.005\n",
      "Running final_fc_width 8 num_conv_layers 1 learningRate 0.01\n",
      "Running final_fc_width 8 num_conv_layers 1 learningRate 0.1\n",
      "Running final_fc_width 8 num_conv_layers 2 learningRate 0.001\n",
      "Running final_fc_width 8 num_conv_layers 2 learningRate 0.005\n",
      "Running final_fc_width 8 num_conv_layers 2 learningRate 0.01\n",
      "Running final_fc_width 8 num_conv_layers 2 learningRate 0.1\n",
      "Running final_fc_width 8 num_conv_layers 3 learningRate 0.001\n",
      "Running final_fc_width 8 num_conv_layers 3 learningRate 0.005\n",
      "Running final_fc_width 8 num_conv_layers 3 learningRate 0.01\n",
      "Running final_fc_width 8 num_conv_layers 3 learningRate 0.1\n",
      "Running final_fc_width 8 num_conv_layers 4 learningRate 0.001\n",
      "Running final_fc_width 8 num_conv_layers 4 learningRate 0.005\n",
      "Running final_fc_width 8 num_conv_layers 4 learningRate 0.01\n",
      "Running final_fc_width 8 num_conv_layers 4 learningRate 0.1\n",
      "Running final_fc_width 8 num_conv_layers 5 learningRate 0.001\n",
      "Running final_fc_width 8 num_conv_layers 5 learningRate 0.005\n",
      "Running final_fc_width 8 num_conv_layers 5 learningRate 0.01\n",
      "Running final_fc_width 8 num_conv_layers 5 learningRate 0.1\n",
      "Running final_fc_width 16 num_conv_layers 1 learningRate 0.001\n",
      "Running final_fc_width 16 num_conv_layers 1 learningRate 0.005\n",
      "Running final_fc_width 16 num_conv_layers 1 learningRate 0.01\n",
      "Running final_fc_width 16 num_conv_layers 1 learningRate 0.1\n",
      "Running final_fc_width 16 num_conv_layers 2 learningRate 0.001\n",
      "Running final_fc_width 16 num_conv_layers 2 learningRate 0.005\n",
      "Running final_fc_width 16 num_conv_layers 2 learningRate 0.01\n",
      "Running final_fc_width 16 num_conv_layers 2 learningRate 0.1\n",
      "Running final_fc_width 16 num_conv_layers 3 learningRate 0.001\n",
      "Running final_fc_width 16 num_conv_layers 3 learningRate 0.005\n",
      "Running final_fc_width 16 num_conv_layers 3 learningRate 0.01\n",
      "Running final_fc_width 16 num_conv_layers 3 learningRate 0.1\n",
      "Running final_fc_width 16 num_conv_layers 4 learningRate 0.001\n",
      "Running final_fc_width 16 num_conv_layers 4 learningRate 0.005\n",
      "Running final_fc_width 16 num_conv_layers 4 learningRate 0.01\n",
      "Running final_fc_width 16 num_conv_layers 4 learningRate 0.1\n",
      "Running final_fc_width 16 num_conv_layers 5 learningRate 0.001\n",
      "Running final_fc_width 16 num_conv_layers 5 learningRate 0.005\n",
      "Running final_fc_width 16 num_conv_layers 5 learningRate 0.01\n",
      "Running final_fc_width 16 num_conv_layers 5 learningRate 0.1\n",
      "Running final_fc_width 32 num_conv_layers 1 learningRate 0.001\n",
      "Running final_fc_width 32 num_conv_layers 1 learningRate 0.005\n",
      "Running final_fc_width 32 num_conv_layers 1 learningRate 0.01\n",
      "Running final_fc_width 32 num_conv_layers 1 learningRate 0.1\n",
      "Running final_fc_width 32 num_conv_layers 2 learningRate 0.001\n",
      "Running final_fc_width 32 num_conv_layers 2 learningRate 0.005\n",
      "Running final_fc_width 32 num_conv_layers 2 learningRate 0.01\n",
      "Running final_fc_width 32 num_conv_layers 2 learningRate 0.1\n",
      "Running final_fc_width 32 num_conv_layers 3 learningRate 0.001\n",
      "Running final_fc_width 32 num_conv_layers 3 learningRate 0.005\n",
      "Running final_fc_width 32 num_conv_layers 3 learningRate 0.01\n",
      "Running final_fc_width 32 num_conv_layers 3 learningRate 0.1\n",
      "Running final_fc_width 32 num_conv_layers 4 learningRate 0.001\n",
      "Running final_fc_width 32 num_conv_layers 4 learningRate 0.005\n",
      "Running final_fc_width 32 num_conv_layers 4 learningRate 0.01\n",
      "Running final_fc_width 32 num_conv_layers 4 learningRate 0.1\n",
      "Running final_fc_width 32 num_conv_layers 5 learningRate 0.001\n",
      "Running final_fc_width 32 num_conv_layers 5 learningRate 0.005\n",
      "Running final_fc_width 32 num_conv_layers 5 learningRate 0.01\n",
      "Running final_fc_width 32 num_conv_layers 5 learningRate 0.1\n",
      "Running final_fc_width 64 num_conv_layers 1 learningRate 0.001\n",
      "Running final_fc_width 64 num_conv_layers 1 learningRate 0.005\n",
      "Running final_fc_width 64 num_conv_layers 1 learningRate 0.01\n",
      "Running final_fc_width 64 num_conv_layers 1 learningRate 0.1\n",
      "Running final_fc_width 64 num_conv_layers 2 learningRate 0.001\n",
      "Running final_fc_width 64 num_conv_layers 2 learningRate 0.005\n",
      "Running final_fc_width 64 num_conv_layers 2 learningRate 0.01\n",
      "Running final_fc_width 64 num_conv_layers 2 learningRate 0.1\n",
      "Running final_fc_width 64 num_conv_layers 3 learningRate 0.001\n",
      "Running final_fc_width 64 num_conv_layers 3 learningRate 0.005\n",
      "Running final_fc_width 64 num_conv_layers 3 learningRate 0.01\n",
      "Running final_fc_width 64 num_conv_layers 3 learningRate 0.1\n",
      "Running final_fc_width 64 num_conv_layers 4 learningRate 0.001\n",
      "Running final_fc_width 64 num_conv_layers 4 learningRate 0.005\n",
      "Running final_fc_width 64 num_conv_layers 4 learningRate 0.01\n",
      "Running final_fc_width 64 num_conv_layers 4 learningRate 0.1\n",
      "Running final_fc_width 64 num_conv_layers 5 learningRate 0.001\n",
      "Running final_fc_width 64 num_conv_layers 5 learningRate 0.005\n",
      "Running final_fc_width 64 num_conv_layers 5 learningRate 0.01\n",
      "Running final_fc_width 64 num_conv_layers 5 learningRate 0.1\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'final_fc_width': [8, 16, 32, 64],\n",
    "    'num_conv_layers': [1, 2, 3, 4, 5],\n",
    "    'learningRate': [0.001, 0.005, 0.01, 0.1],\n",
    "}\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "param_combinations = list(itertools.product(*param_grid.values()))\n",
    "\n",
    "final_fc_width = []\n",
    "num_conv_layers = []\n",
    "learningRate = []\n",
    "accuracy = []\n",
    "\n",
    "# Print the combinations\n",
    "for params in param_combinations:\n",
    "    print(\"Running final_fc_width\", params[0], \"num_conv_layers\", params[1], \"learningRate\", params[2])\n",
    "    # Create an instance of MyModel\n",
    "    model = BaseCNN(num_conv_layers = params[1], final_fc_width = params[0])\n",
    "    model.fit(X_train, X_val, y_train, y_val, learningRate = params[2], num_epochs = 200)\n",
    "    accuracy.append(model.predict_score(X_test, y_test))\n",
    "    final_fc_width.append(params[0])\n",
    "    num_conv_layers.append(params[1])\n",
    "    learningRate.append(params[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2dbae864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>final_fc_width</th>\n",
       "      <th>num_conv_layers</th>\n",
       "      <th>learningRate</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    final_fc_width  num_conv_layers  learningRate accuracy\n",
       "0                8                1         0.001    0.947\n",
       "1                8                1         0.005    0.988\n",
       "2                8                1         0.010    0.979\n",
       "3                8                1         0.100     0.97\n",
       "4                8                2         0.001    0.997\n",
       "5                8                2         0.005    0.997\n",
       "6                8                2         0.010    0.993\n",
       "7                8                2         0.100    0.499\n",
       "8                8                3         0.001    0.989\n",
       "9                8                3         0.005    0.995\n",
       "10               8                3         0.010    0.499\n",
       "11               8                3         0.100    0.664\n",
       "12               8                4         0.001    0.993\n",
       "13               8                4         0.005    0.988\n",
       "14               8                4         0.010    0.992\n",
       "15               8                4         0.100    0.499\n",
       "16               8                5         0.001    0.984\n",
       "17               8                5         0.005    0.499\n",
       "18               8                5         0.010    0.994\n",
       "19               8                5         0.100    0.501\n",
       "20              16                1         0.001    0.838\n",
       "21              16                1         0.005    0.941\n",
       "22              16                1         0.010    0.499\n",
       "23              16                1         0.100    0.904\n",
       "24              16                2         0.001    0.997\n",
       "25              16                2         0.005    0.998\n",
       "26              16                2         0.010    0.998\n",
       "27              16                2         0.100    0.499\n",
       "28              16                3         0.001    0.993\n",
       "29              16                3         0.005    0.993\n",
       "30              16                3         0.010    0.995\n",
       "31              16                3         0.100    0.499\n",
       "32              16                4         0.001    0.992\n",
       "33              16                4         0.005    0.985\n",
       "34              16                4         0.010    0.983\n",
       "35              16                4         0.100    0.499\n",
       "36              16                5         0.001    0.992\n",
       "37              16                5         0.005    0.995\n",
       "38              16                5         0.010    0.995\n",
       "39              16                5         0.100    0.499\n",
       "40              32                1         0.001    0.963\n",
       "41              32                1         0.005    0.986\n",
       "42              32                1         0.010    0.863\n",
       "43              32                1         0.100    0.927\n",
       "44              32                2         0.001    0.995\n",
       "45              32                2         0.005     0.99\n",
       "46              32                2         0.010    0.997\n",
       "47              32                2         0.100    0.972\n",
       "48              32                3         0.001    0.995\n",
       "49              32                3         0.005    0.991\n",
       "50              32                3         0.010    0.994\n",
       "51              32                3         0.100    0.499\n",
       "52              32                4         0.001    0.989\n",
       "53              32                4         0.005    0.992\n",
       "54              32                4         0.010     0.99\n",
       "55              32                4         0.100    0.499\n",
       "56              32                5         0.001     0.98\n",
       "57              32                5         0.005    0.989\n",
       "58              32                5         0.010    0.992\n",
       "59              32                5         0.100    0.499\n",
       "60              64                1         0.001     0.96\n",
       "61              64                1         0.005    0.973\n",
       "62              64                1         0.010    0.931\n",
       "63              64                1         0.100    0.501\n",
       "64              64                2         0.001    0.993\n",
       "65              64                2         0.005    0.995\n",
       "66              64                2         0.010    0.995\n",
       "67              64                2         0.100    0.499\n",
       "68              64                3         0.001    0.996\n",
       "69              64                3         0.005    0.989\n",
       "70              64                3         0.010    0.995\n",
       "71              64                3         0.100    0.499\n",
       "72              64                4         0.001     0.99\n",
       "73              64                4         0.005    0.993\n",
       "74              64                4         0.010    0.988\n",
       "75              64                4         0.100    0.499\n",
       "76              64                5         0.001    0.991\n",
       "77              64                5         0.005    0.993\n",
       "78              64                5         0.010    0.989\n",
       "79              64                5         0.100    0.499"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "param_combinations = {\n",
    "    'final_fc_width': final_fc_width,\n",
    "    'num_conv_layers': num_conv_layers,\n",
    "    'learningRate':learningRate,\n",
    "    'accuracy': accuracy\n",
    "}\n",
    "\n",
    "DataSettignsdf = pd.DataFrame(param_combinations)\n",
    "display(DataSettignsdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e59fcb13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>final_fc_width</th>\n",
       "      <th>num_conv_layers</th>\n",
       "      <th>learningRate</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    final_fc_width  num_conv_layers  learningRate accuracy\n",
       "25              16                2         0.005    0.998"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexMax = np.argmax(DataSettignsdf['accuracy'].values)\n",
    "DataSettignsdf.iloc[[indexMax]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07093b78",
   "metadata": {},
   "source": [
    "Lets explore the data now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "02fe43a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'final_fc_width': [8, 16, 32, 64],\n",
       " 'num_conv_layers': [1, 2, 3, 4, 5],\n",
       " 'learningRate': [0.001, 0.005, 0.01, 0.1]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "222e9bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 4 artists>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "score_avg = []\n",
    "for item in param_grid[\"final_fc_width\"]:\n",
    "    score_avg.append(DataSettignsdf[DataSettignsdf[\"final_fc_width\"] == item][\"accuracy\"].mean())\n",
    "\n",
    "plt.bar(param_grid[\"final_fc_width\"], score_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032e8e0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
